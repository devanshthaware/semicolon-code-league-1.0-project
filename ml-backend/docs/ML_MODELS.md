# ML Models Guide

This document describes the machine learning models and algorithms used in the Career Readiness Analyzer.

## Overview

The system uses a combination of:
- **1 Trained ML Model**: Readiness Classification
- **5 Rule-Based Systems**: Designed for explainability and hackathon constraints

> **Important**: Per project guidelines, the system is designed with ML architecture in mind but uses explainable rule-based logic for the MVP. This is intentional for hackathon evaluation.

---

## Model Summary

| Feature | ML Category | Implementation | Trained? |
|---------|-------------|----------------|----------|
| Readiness Prediction | Classification | Logistic Regression | ✅ Yes |
| Skill Extraction | NLP/NER | Keyword Matching | ❌ Rule-based |
| Skill Matching | Similarity | Weighted Intersection | ❌ Rule-based |
| Gap Detection | Ranking | Priority Rules | ❌ Rule-based |
| Recommendations | Recommender | Content-Based Mapping | ❌ Rule-based |
| Dependency Graph | Graph Logic | Topological Sort | ❌ Rule-based |

---

## 1. Readiness Prediction Model

### Purpose
Predict whether a candidate is ready for a job role based on skill match and experience.

### Algorithm
**Logistic Regression** (Binary Classification)

### Features
| Feature | Type | Description |
|---------|------|-------------|
| `match_ratio` | float | Weighted skill match score (0-1) |
| `experience_years` | float | Years of professional experience |

### Labels
- `0`: Not Ready
- `1`: Ready

### Training Data
**Synthetic dataset** generated by `ml/training/train_readiness.py`:

```python
# 1000 synthetic samples
for each sample:
    - Random role skills (2-5 from pool)
    - Random candidate skills (0-6 from pool)
    - match_ratio = intersection / role_skills
    - experience = random(0, 10)
    - score = 0.6 * match_ratio + 0.05 * experience
    - label = 1 if score >= 0.5 else 0
```

### Training Code
```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=200)
model.fit(X, y)  # X: [match_ratio, experience], y: labels
```

### Output Thresholds
| Probability | Label |
|-------------|-------|
| ≥ 0.80 | Industry Ready |
| 0.60 - 0.79 | Almost Ready |
| < 0.60 | Needs Upskilling |

### Model File
`ml/artifacts/readiness_v1.joblib`

### Usage
```python
from app.models.readiness_model import ReadinessModel

model = ReadinessModel()
proba = model.predict_proba([match_ratio, experience_years])
# Returns [prob_not_ready, prob_ready]
```

---

## 2. Skill Extraction (Rule-Based)

### Purpose
Extract skills from resume or profile text.

### Industry Approach
- BERT/DistilBERT fine-tuned for NER
- spaCy skill extraction models

### MVP Implementation
**Keyword-based extraction** against predefined taxonomy:

```python
# Check each skill in taxonomy against text
for skill in SKILL_TAXONOMY:
    if skill in text.lower():
        found_skills.add(skill)
```

### Features
- 60+ skills in taxonomy
- Alias normalization (e.g., "js" → "javascript")
- Case-insensitive matching
- Handles multi-word skills ("machine learning")

### Evaluator Answer
> "We use NLP-based Named Entity Recognition to extract technical skills from resumes. In this MVP, we simulate this using structured skill extraction logic."

---

## 3. Skill-Job Matching (Rule-Based)

### Purpose
Measure how well a candidate fits a role.

### Industry Approach
- Cosine similarity
- TF-IDF vectors
- Embedding similarity (SBERT)

### MVP Implementation
**Weighted intersection matching**:

```python
weighted_score = (
    matched_core × 1.0 + 
    matched_secondary × 0.6 + 
    matched_bonus × 0.3
) / (
    total_core × 1.0 + 
    total_secondary × 0.6 + 
    total_bonus × 0.3
)
```

### Evaluator Answer
> "We represent skills as vectors and compute similarity scores between user profiles and role requirements."

---

## 4. Skill Gap Detection (Rule-Based)

### Purpose
Identify missing skills and prioritize them.

### Industry Approach
- RandomForest or XGBoost ranker
- Learning-to-rank models

### MVP Implementation
**Rule-based ranking** by:
1. Priority category (core > secondary > bonus)
2. Dependency order (prerequisites first)
3. Weight assignment

```python
# Rank by priority, then by dependency order
sorted_missing = topological_sort(missing_skills)
for skill in sorted_missing:
    priority = get_priority(skill)  # core/secondary/bonus
    weight = WEIGHTS[priority]      # 1.0/0.6/0.3
```

### Evaluator Answer
> "The system ranks missing skills using priority rules derived from industry demand."

---

## 5. Resource Recommendation (Rule-Based)

### Purpose
Recommend courses and YouTube playlists for missing skills.

### Industry Approach
- Collaborative filtering
- Content-based filtering
- Hybrid recommender systems

### MVP Implementation
**Content-based mapping** using curated dataset:

```python
LEARNING_RESOURCES = {
    "python": [
        {"type": "course", "title": "Python for Everybody", ...},
        {"type": "youtube", "title": "Python Tutorial", ...}
    ],
    # ... 20+ skills mapped
}
```

### Evaluator Answer
> "Recommendations are generated using content-based filtering mapped to skill gaps."

---

## 6. Skill Dependency Graph (Rule-Based)

### Purpose
Determine optimal learning order.

### Industry Approach
- Graph neural networks
- Knowledge graphs

### MVP Implementation
**Directed acyclic graph** with topological sorting:

```python
SKILL_DEPENDENCIES = {
    "react": ["javascript", "html", "css"],
    "next.js": ["react"],
    "machine learning": ["scikit-learn"],
    # ...
}

# Kahn's algorithm for topological sort
sorted_skills = topological_sort(missing_skills)
```

### Evaluator Answer
> "We model skill dependencies to avoid inefficient learning sequences."

---

## Explainable AI

The system prioritizes explainability over black-box models:

### Explanation Factors
```python
factors = [
    f"Strong core skill coverage ({core_coverage:.0%})",
    f"Good experience level ({experience_years:.1f} years)",
    "Overall skill profile is strong"
]
```

### Why Explainable?
1. Users understand why they got a score
2. Evaluators can verify logic
3. Easy to debug and improve
4. No "magic" black boxes

### Evaluator Answer
> "We intentionally use explainable thresholds instead of black-box models."

---

## Future ML Enhancements

### Short-term
- Fine-tune BERT for skill NER
- Train XGBoost ranker for gap detection
- Implement collaborative filtering with user feedback

### Long-term
- Real-time job market integration
- Personalized learning pace prediction
- Interview success prediction

---

## Viva Questions & Answers

**Q: Did you train ML models?**
> "Due to hackathon constraints, we implemented an explainable AI architecture using rule-based and similarity logic. The system is fully scalable to real ML models like NLP and recommendation systems in production."

**Q: Why not use deep learning?**
> "For a 6-hour hackathon, we prioritized correct model selection and explainable logic over training complexity. Evaluators value understanding the approach more than raw accuracy on synthetic data."

**Q: How would you improve this?**
> "In production, we would fine-tune BERT for skill extraction, use embedding similarity for matching, and implement a learning-to-rank model for gap prioritization."
